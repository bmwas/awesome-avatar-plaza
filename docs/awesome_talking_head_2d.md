# 2D Talking Head

- [2D Talking Head](#2d-talking-head)
	- [Papers](#papers)
	- [Extra](#extra)
	- [Tools](#tools)

  
## Papers
- **Learning Dynamic Tetrahedra for High-Quality Talking Head Synthesis**, `arXiv, 2402.17364`, [arxiv](http://arxiv.org/abs/2402.17364v1), [pdf](http://arxiv.org/pdf/2402.17364v1.pdf), cication: [**-1**](None)

	 *Zicheng Zhang, Ruobing Zheng, Ziwen Liu, Congying Han, Tianqi Li, Meng Wang, Tiande Guo, Jingdong Chen, Bonan Li, Ming Yang*
- **EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with
  Audio2Video Diffusion Model under Weak Conditions**, `arXiv, 2402.17485`, [arxiv](http://arxiv.org/abs/2402.17485v1), [pdf](http://arxiv.org/pdf/2402.17485v1.pdf), cication: [**-1**](None)

	 *Linrui Tian, Qi Wang, Bang Zhang, Liefeng Bo*
- [**LipFD**](https://github.com/AaronComo/LipFD) - AaronComo ![Star](https://img.shields.io/github/stars/AaronComo/LipFD.svg?style=social&label=Star)

	 *This repository contains the codes of "Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-syncing DeepFakes".*
- [DR2: Disentangled Recurrent Representation Learning for Data-efficient Speech Video Synthesis](https://openaccess.thecvf.com/content/WACV2024/papers/Zhang_DR2_Disentangled_Recurrent_Representation_Learning_for_Data-Efficient_Speech_Video_Synthesis_WACV_2024_paper.pdf)
- **EmoTalker: Emotionally Editable Talking Face Generation via Diffusion
  Model**, `arXiv, 2401.08049`, [arxiv](http://arxiv.org/abs/2401.08049v1), [pdf](http://arxiv.org/pdf/2401.08049v1.pdf), cication: [**-1**](None)

	 *Bingyuan Zhang, Xulong Zhang, Ning Cheng, Jun Yu, Jing Xiao, Jianzong Wang*
- **Real3D-Portrait: One-shot Realistic 3D Talking Portrait Synthesis**, `arXiv, 2401.08503`, [arxiv](http://arxiv.org/abs/2401.08503v2), [pdf](http://arxiv.org/pdf/2401.08503v2.pdf), cication: [**-1**](None)

	 *Zhenhui Ye, Tianyun Zhong, Yi Ren, Jiaqi Yang, Weichuang Li, Jiawei Huang, Ziyue Jiang, Jinzheng He, Rongjie Huang, Jinglin Liu* · ([real3dportrait.github](https://real3dportrait.github.io/))
- **Jump Cut Smoothing for Talking Heads**, `arXiv, 2401.04718`, [arxiv](http://arxiv.org/abs/2401.04718v2), [pdf](http://arxiv.org/pdf/2401.04718v2.pdf), cication: [**-1**](None)

	 *Xiaojuan Wang, Taesung Park, Yang Zhou, Eli Shechtman, Richard Zhang*
- **VectorTalker: SVG Talking Face Generation with Progressive Vectorisation**, `arXiv, 2312.11568`, [arxiv](http://arxiv.org/abs/2312.11568v1), [pdf](http://arxiv.org/pdf/2312.11568v1.pdf), cication: [**-1**](None)

	 *Hao Hu, Xuan Wang, Jingxiang Sun, Yanbo Fan, Yu Guo, Caigui Jiang*
- **AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head
  Synthesis**, `arXiv, 2312.10921`, [arxiv](http://arxiv.org/abs/2312.10921v1), [pdf](http://arxiv.org/pdf/2312.10921v1.pdf), cication: [**-1**](None)

	 *Dongze Li, Kang Zhao, Wei Wang, Bo Peng, Yingya Zhang, Jing Dong, Tieniu Tan*
- **DreamTalk: When Expressive Talking Head Generation Meets Diffusion
  Probabilistic Models**, `arXiv, 2312.09767`, [arxiv](http://arxiv.org/abs/2312.09767v1), [pdf](http://arxiv.org/pdf/2312.09767v1.pdf), cication: [**-1**](None)

	 *Yifeng Ma, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yingya Zhang, Zhidong Deng* · ([dreamtalk-project.github](https://dreamtalk-project.github.io/)) · ([dreamtalk](https://github.com/ali-vilab/dreamtalk) - ali-vilab) ![Star](https://img.shields.io/github/stars/ali-vilab/dreamtalk.svg?style=social&label=Star)
- **FT2TF: First-Person Statement Text-To-Talking Face Generation**, `arXiv, 2312.05430`, [arxiv](http://arxiv.org/abs/2312.05430v1), [pdf](http://arxiv.org/pdf/2312.05430v1.pdf), cication: [**-1**](None)

	 *Xingjian Diao, Ming Cheng, Wayner Barrios, SouYoung Jin*
- **GMTalker: Gaussian Mixture based Emotional talking video Portraits**, `arXiv, 2312.07669`, [arxiv](http://arxiv.org/abs/2312.07669v1), [pdf](http://arxiv.org/pdf/2312.07669v1.pdf), cication: [**-1**](None)

	 *Yibo Xia, Lizhen Wang, Xiang Deng, Xiaoyan Luo, Yebin Liu* · ([bob35buaa.github](https://bob35buaa.github.io/GMTalker))
- [GSmoothFace: Generalized Smooth Talking Face Generation via Fine Grained 3D Face Guidance](https://zhanghm1995.github.io/GSmoothFace/)

	 · ([zhanghm1995.github](https://zhanghm1995.github.io/GSmoothFace/))
- **Talking Head(?) Anime from a Single Image 4: Improved Model and Its
  Distillation**, `arXiv, 2311.17409`, [arxiv](http://arxiv.org/abs/2311.17409v2), [pdf](http://arxiv.org/pdf/2311.17409v2.pdf), cication: [**-1**](None)

	 *Pramook Khungurn*
- **SingingHead: A Large-scale 4D Dataset for Singing Head Animation**, `arXiv, 2312.04369`, [arxiv](http://arxiv.org/abs/2312.04369v2), [pdf](http://arxiv.org/pdf/2312.04369v2.pdf), cication: [**-1**](None)

	 *Sijing Wu, Yunhao Li, Weitian Zhang, Jun Jia, Yucheng Zhu, Yichao Yan, Guangtao Zhai*
- **VividTalk: One-Shot Audio-Driven Talking Head Generation Based on 3D
  Hybrid Prior**, `arXiv, 2312.01841`, [arxiv](http://arxiv.org/abs/2312.01841v2), [pdf](http://arxiv.org/pdf/2312.01841v2.pdf), cication: [**-1**](None)

	 *Xusen Sun, Longhao Zhang, Hao Zhu, Peng Zhang, Bang Zhang, Xinya Ji, Kangneng Zhou, Daiheng Gao, Liefeng Bo, Xun Cao* · ([mp.weixin.qq](https://mp.weixin.qq.com/s?__biz=MzI3MTA0MTk1MA==&mid=2652418245&idx=4&sn=7776f90ea245adbd520ba616d7f24cc5))
- **SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis**, `arXiv, 2311.17590`, [arxiv](http://arxiv.org/abs/2311.17590v1), [pdf](http://arxiv.org/pdf/2311.17590v1.pdf), cication: [**-1**](None)

	 *Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan*
- **GAIA: Zero-shot Talking Avatar Generation**, `arXiv, 2311.15230`, [arxiv](http://arxiv.org/abs/2311.15230v1), [pdf](http://arxiv.org/pdf/2311.15230v1.pdf), cication: [**-1**](None)

	 *Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang, Jialiang Zhu, Kaikai An, Leyi Li, Xu Tan, Chunyu Wang, Han Hu* · ([microsoft.github](https://microsoft.github.io/GAIA))
- **SyncTalk: The Devil is in the Synchronization for Talking Head Synthesis**, `arXiv, 2311.17590`, [arxiv](http://arxiv.org/abs/2311.17590v1), [pdf](http://arxiv.org/pdf/2311.17590v1.pdf), cication: [**-1**](None)

	 *Ziqiao Peng, Wentao Hu, Yue Shi, Xiangyu Zhu, Xiaomei Zhang, Hao Zhao, Jun He, Hongyan Liu, Zhaoxin Fan* · ([SyncTalk](https://github.com/ZiqiaoPeng/SyncTalk) - ZiqiaoPeng) ![Star](https://img.shields.io/github/stars/ZiqiaoPeng/SyncTalk.svg?style=social&label=Star) · ([ziqiaopeng.github](https://ziqiaopeng.github.io/synctalk/))
- **3D-Aware Talking-Head Video Motion Transfer**, `proceedings of the ieee/cvf winter conference on applications …, 2024`, [arxiv](http://arxiv.org/abs/2311.02549v1), [pdf](http://arxiv.org/pdf/2311.02549v1.pdf), cication: [**-1**](None)

	 *Haomiao Ni, Jiachen Liu, Yuan Xue, Sharon X. Huang*
- **High-Fidelity and Freely Controllable Talking Head Video Generation**, `CVPR, 2023`, [arxiv](http://arxiv.org/abs/2304.10168v2), [pdf](http://arxiv.org/pdf/2304.10168v2.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=12571688082953986149&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yue Gao, Yuan Zhou, Jinglu Wang, Xiao Li, Xiang Ming, Yan Lu*
- **Continuously Controllable Facial Expression Editing in Talking Face
  Videos**, `ieee transactions on affective computing, 2023`, [arxiv](http://arxiv.org/abs/2209.08289v2), [pdf](http://arxiv.org/pdf/2209.08289v2.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=7198079248282369738&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhiyao Sun, Yu-Hui Wen, Tian Lv, Yanan Sun, Ziyang Zhang, Yaoyuan Wang, Yong-Jin Liu*
- **Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation**, `ICCV, 2023`, [arxiv](http://arxiv.org/abs/2309.04946v2), [pdf](http://arxiv.org/pdf/2309.04946v2.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=8927617372001492663&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun, Yi Yang*
- **HyperLips: Hyper Control Lips with High Resolution Decoder for Talking
  Face Generation**, `arXiv, 2310.05720`, [arxiv](http://arxiv.org/abs/2310.05720v3), [pdf](http://arxiv.org/pdf/2310.05720v3.pdf), cication: [**-1**](None)

	 *Yaosen Chen, Yu Yao, Zhiqiang Li, Wei Wang, Yanru Zhang, Han Yang, Xuming Wen*
- **OSM-Net: One-to-Many One-shot Talking Head Generation with Spontaneous
  Head Motions**, `arXiv, 2309.16148`, [arxiv](http://arxiv.org/abs/2309.16148v1), [pdf](http://arxiv.org/pdf/2309.16148v1.pdf), cication: [**-1**](None)

	 *Jin Liu, Xi Wang, Xiaomeng Fu, Yesheng Chai, Cai Yu, Jiao Dai, Jizhong Han*
- **DT-NeRF: Decomposed Triplane-Hash Neural Radiance Fields for
  High-Fidelity Talking Portrait Synthesis**, `arXiv, 2309.07752`, [arxiv](http://arxiv.org/abs/2309.07752v1), [pdf](http://arxiv.org/pdf/2309.07752v1.pdf), cication: [**-1**](None)

	 *Yaoyu Su, Shaohui Wang, Haoqian Wang*
- **VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style
  Transfer**, `ICCV, 2023`, [arxiv](http://arxiv.org/abs/2308.04830v2), [pdf](http://arxiv.org/pdf/2308.04830v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=4456463487657102707&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Liyang Chen, Zhiyong Wu, Runnan Li, Weihong Bao, Jun Ling, Xu Tan, Sheng Zhao*
- **Text-to-Video: a Two-stage Framework for Zero-shot Identity-agnostic
  Talking-head Generation**, `arXiv, 2308.06457`, [arxiv](http://arxiv.org/abs/2308.06457v1), [pdf](http://arxiv.org/pdf/2308.06457v1.pdf), cication: [**-1**](None)

	 *Zhichao Wang, Mengyu Dai, Keld Lundgaard*
- **HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and
  Retarget Faces**, `ICCV, 2023`, [arxiv](http://arxiv.org/abs/2307.10797v1), [pdf](http://arxiv.org/pdf/2307.10797v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=4487953749834409917&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Stella Bounareli, Christos Tzelepis, Vasileios Argyriou, Ioannis Patras, Georgios Tzimiropoulos* · ([stelabou.github](https://stelabou.github.io/hyperreenact.github.io/)) · ([HyperReenact](https://github.com/StelaBou/HyperReenact) - StelaBou) ![Star](https://img.shields.io/github/stars/StelaBou/HyperReenact.svg?style=social&label=Star)
- **Implicit Identity Representation Conditioned Memory Compensation Network
  for Talking Head video Generation**, `ICCV, 2023`, [arxiv](http://arxiv.org/abs/2307.09906v3), [pdf](http://arxiv.org/pdf/2307.09906v3.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=15551491585011126437&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Fa-Ting Hong, Dan Xu* · ([ICCV2023-MCNET](https://github.com/harlanhong/ICCV2023-MCNET) - harlanhong) ![Star](https://img.shields.io/github/stars/harlanhong/ICCV2023-MCNET.svg?style=social&label=Star) · ([harlanhong.github](https://harlanhong.github.io/publications/mcnet.html))
- **MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions**, `ICCV, 2023`, [arxiv](http://arxiv.org/abs/2307.10008v1), [pdf](http://arxiv.org/pdf/2307.10008v1.pdf), cication: [**3**](https://scholar.google.com/scholar?cites=12958348119986055500&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yunfei Liu, Lijian Lin, Fei Yu, Changyin Zhou, Yu Li* · ([liuyunfei](https://liuyunfei.net/projects/iccv23-moda/)) · ([MODA](https://github.com/DreamtaleCore/MODA) - DreamtaleCore) ![Star](https://img.shields.io/github/stars/DreamtaleCore/MODA.svg?style=social&label=Star)
- **Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking
  Portrait Synthesis**, `ICCV, 2023`, [arxiv](http://arxiv.org/abs/2307.09323v2), [pdf](http://arxiv.org/pdf/2307.09323v2.pdf), cication: [**8**](https://scholar.google.com/scholar?cites=3191872383711910284&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jiahe Li, Jiawei Zhang, Xiao Bai, Jun Zhou, Lin Gu* · ([ER-NeRF](https://github.com/Fictionarry/ER-NeRF) - Fictionarry) ![Star](https://img.shields.io/github/stars/Fictionarry/ER-NeRF.svg?style=social&label=Star) · ([youtu](https://youtu.be/Gc2d3Z8MMuI))
- **Ada-TTA: Towards Adaptive High-Quality Text-to-Talking Avatar Synthesis**, `arXiv, 2306.03504`, [arxiv](http://arxiv.org/abs/2306.03504v2), [pdf](http://arxiv.org/pdf/2306.03504v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=2525485338562235164&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, Chen Zhang, Xiang Yin, Zejun Ma, Zhou Zhao* · ([mega-tts.github](https://mega-tts.github.io/demo-page/))
- **Instruct-NeuralTalker: Editing Audio-Driven Talking Radiance Fields with
  Instructions**, `arXiv, 2306.10813`, [arxiv](http://arxiv.org/abs/2306.10813v2), [pdf](http://arxiv.org/pdf/2306.10813v2.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=5092853423478459228&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Yuqi Sun, Ruian He, Weimin Tan, Bo Yan*
- **Unsupervised Learning of Style-Aware Facial Animation from Real Acting
  Performances**, `arXiv, 2306.10006`, [arxiv](http://arxiv.org/abs/2306.10006v3), [pdf](http://arxiv.org/pdf/2306.10006v3.pdf), cication: [**-1**](None)

	 *Wolfgang Paier, Anna Hilsmann, Peter Eisert*
- **Parametric Implicit Face Representation for Audio-Driven Facial
  Reenactment**, `CVPR, 2023`, [arxiv](http://arxiv.org/abs/2306.07579v1), [pdf](http://arxiv.org/pdf/2306.07579v1.pdf), cication: [**4**](https://scholar.google.com/scholar?cites=17513468456716565773&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ricong Huang, Peiwen Lai, Yipeng Qin, Guanbin Li*
- **Generalizable One-shot Neural Head Avatar**, `NeurIPS, 2024`, [arxiv](http://arxiv.org/abs/2306.08768v1), [pdf](http://arxiv.org/pdf/2306.08768v1.pdf), cication: [**6**](https://scholar.google.com/scholar?cites=3086679006547043034&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Xueting Li, Shalini De Mello, Sifei Liu, Koki Nagano, Umar Iqbal, Jan Kautz* · ([research.nvidia](https://research.nvidia.com/labs/lpr/one-shot-avatar/)) · ([GOHA](https://github.com/NVlabs/GOHA) - NVlabs) ![Star](https://img.shields.io/github/stars/NVlabs/GOHA.svg?style=social&label=Star)
- **Emotional Speech-Driven Animation with Content-Emotion Disentanglement**, `arXiv, 2306.08990`, [arxiv](http://arxiv.org/abs/2306.08990v2), [pdf](http://arxiv.org/pdf/2306.08990v2.pdf), cication: [**-1**](None)

	 *Radek Daněček, Kiran Chhatre, Shashank Tripathi, Yandong Wen, Michael J. Black, Timo Bolkart*
- **Laughing Matters: Introducing Laughing-Face Generation using Diffusion
  Models**, `arXiv, 2305.08854`, [arxiv](http://arxiv.org/abs/2305.08854v2), [pdf](http://arxiv.org/pdf/2305.08854v2.pdf), cication: [**-1**](None)

	 *Antoni Bigata Casademunt, Rodrigo Mira, Nikita Drobyshev, Konstantinos Vougioukas, Stavros Petridis, Maja Pantic*

## Extra
- **Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural
  Rendering Priors**, `arXiv, 2401.06126`, [arxiv](http://arxiv.org/abs/2401.06126v1), [pdf](http://arxiv.org/pdf/2401.06126v1.pdf), cication: [**-1**](None)

	 *Jack Saunders, Vinay Namboodiri* · ([dubbingforeveryone.github](https://dubbingforeveryone.github.io/))
- **RADIO: Reference-Agnostic Dubbing Video Synthesis**, `proceedings of the ieee/cvf winter conference on applications …, 2024`, [arxiv](http://arxiv.org/abs/2309.01950v2), [pdf](http://arxiv.org/pdf/2309.01950v2.pdf), cication: [**-1**](None)

	 *Dongyeun Lee, Chaewon Kim, Sangjoon Yu, Jaejun Yoo, Gyeong-Moon Park*
- **Learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking
  Heads Generation**, `arXiv, 2306.01415`, [arxiv](http://arxiv.org/abs/2306.01415v2), [pdf](http://arxiv.org/pdf/2306.01415v2.pdf), cication: [**-1**](None)

	 *Federico Nocentini, Claudio Ferrari, Stefano Berretti*
- **FACTS: Facial Animation Creation using the Transfer of Styles**, `arXiv, 2307.09480`, [arxiv](http://arxiv.org/abs/2307.09480v1), [pdf](http://arxiv.org/pdf/2307.09480v1.pdf), cication: [**-1**](None)

	 *Jack Saunders, Steven Caulkin, Vinay Namboodiri*
- **FTFDNet: Learning to Detect Talking Face Video Manipulation with
  Tri-Modality Interaction**, `arXiv, 2307.03990`, [arxiv](http://arxiv.org/abs/2307.03990v1), [pdf](http://arxiv.org/pdf/2307.03990v1.pdf), cication: [**-1**](None)

	 *Ganglai Wang, Peng Zhang, Junwen Xiong, Feihan Yang, Wei Huang, Yufei Zha*
- **Audio-driven Talking Face Generation by Overcoming Unintended
  Information Flow**, `arXiv, 2307.09368`, [arxiv](http://arxiv.org/abs/2307.09368v2), [pdf](http://arxiv.org/pdf/2307.09368v2.pdf), cication: [**-1**](None)

	 *Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Bärmann, Hazim Kemal Ekenel, Alexander Waibel*
- **Facial Expression Re-targeting from a Single Character**, `arXiv, 2306.12188`, [arxiv](http://arxiv.org/abs/2306.12188v1), [pdf](http://arxiv.org/pdf/2306.12188v1.pdf), cication: [**1**](https://scholar.google.com/scholar?cites=11476307519316802770&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Ariel Larey, Omri Asraf, Adam Kelder, Itzik Wilf, Ofer Kruzel, Nati Daniel*
- **Emotional Talking Head Generation based on Memory-Sharing and
  Attention-Augmented Networks**, `arXiv, 2306.03594`, [arxiv](http://arxiv.org/abs/2306.03594v1), [pdf](http://arxiv.org/pdf/2306.03594v1.pdf), cication: [**2**](https://scholar.google.com/scholar?cites=11464875913405254553&as_sdt=2005&sciodt=0,5&hl=en&oe=ASCII)

	 *Jianrong Wang, Yaxin Zhao, Li Liu, Tianyi Xu, Qi Li, Sen Li*
- [UniFLG: Unified Facial Landmark Generator from Text or Speech](https://rinnakk.github.io/research/publications/UniFLG/)

## Tools
- [**pyvideotrans**](https://github.com/jianchang512/pyvideotrans) - jianchang512 ![Star](https://img.shields.io/github/stars/jianchang512/pyvideotrans.svg?style=social&label=Star)

	 *Translate the video from one language to another and add dubbing. 将视频从一种语言翻译为另一种语言，并添加配音*